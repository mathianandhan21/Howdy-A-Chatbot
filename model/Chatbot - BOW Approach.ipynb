{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS-b_j6ZdPni",
        "outputId": "c4f74c11-68ad-4a8d-b3a1-a0e1a422033c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import json\n",
        "import string\n",
        "import random \n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import tensorflow as tf \n",
        "from tensorflow.keras import Sequential \n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.corpus import wordnet\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing lemmatizer to get stem of words\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "q7yiCO_gg4_s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'caring'\n",
        "print(lemmatizer.lemmatize(word, pos='v'))"
      ],
      "metadata": {
        "id": "X0gx3NcKhW9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c3f29e-7149-449a-affc-b67ea3623138"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "care\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json (r'/content/file.json')"
      ],
      "metadata": {
        "id": "8QaLz1_GoDVn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "doc_X = []\n",
        "doc_y = []\n",
        "classes = []"
      ],
      "metadata": {
        "id": "7QhSA3wS84QL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for intent in df[\"intents\"]:\n",
        "  for pattern in intent[\"patterns\"]:\n",
        "    token = nltk.word_tokenize(pattern)\n",
        "    words.extend(token)\n",
        "    doc_X.append(pattern)\n",
        "    doc_y.append(intent[\"tag\"])\n",
        "\n",
        "    if intent[\"tag\"] not in classes:\n",
        "      classes.append(intent[\"tag\"])"
      ],
      "metadata": {
        "id": "qsxVKcA6pP9_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [lemmatizer.lemmatize((word)) for word in words if word not in string.punctuation]"
      ],
      "metadata": {
        "id": "B15HeCQVqu37"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))"
      ],
      "metadata": {
        "id": "qgE_rvj4tx7E"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK-nKP0E_RRg",
        "outputId": "0a63bd88-3ddd-4524-bede-41c4c91b0219"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training = []\n",
        "out_empty = [0] * len(classes)\n",
        "for idx, doc in enumerate(doc_X):\n",
        "    bow = []\n",
        "    text = lemmatizer.lemmatize(doc.lower())\n",
        "    for word in words:\n",
        "        bow.append(1) if word.lower() in text else bow.append(0)\n",
        "        output_row = list(out_empty)\n",
        "        output_row[classes.index(doc_y[idx])] = 1\n",
        "        training.append([bow, output_row])"
      ],
      "metadata": {
        "id": "iF8jH0YhzIHQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training"
      ],
      "metadata": {
        "id": "WtTs71ksCEOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)"
      ],
      "metadata": {
        "id": "kux22--r9gUW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = np.array(list(training[:,0]))\n",
        "train_y = np.array(list(training[:,1]))"
      ],
      "metadata": {
        "id": "1kXHS7cgzZ9I"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes"
      ],
      "metadata": {
        "id": "MD2QUmzeJLAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining some parameters\n",
        "input_shape = (len(train_x[0]),)\n",
        "output_shape = len(train_y[0])\n",
        "epochs = 200\n",
        "# the deep learning model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=input_shape, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(output_shape, activation = \"softmax\"))\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=[\"accuracy\"])\n",
        "print(model.summary())\n",
        "model.fit(x=train_x, y=train_y, epochs=200, verbose=1)"
      ],
      "metadata": {
        "id": "mg-yXP-gA5tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_class(message,wordstring,classes):\n",
        "  text = lemmatizer.lemmatize(message.lower())\n",
        "  bow_list = []\n",
        "  for word in words:\n",
        "    bow_list.append(1) if word.lower() in text else bow_list.append(0)\n",
        "  result = model.predict(np.array([bow_list]))[0]\n",
        "  thresh = 0.2\n",
        "  y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n",
        "  y_pred.sort(key=lambda x: x[1], reverse=True) \n",
        "  y_pred = y_pred[:1]\n",
        "  lables = classes[y_pred[0][0]]\n",
        "  for intent in df[\"intents\"]:\n",
        "    if intent[\"tag\"] == lables:\n",
        "      result = random.choice(intent[\"responses\"])\n",
        "      break\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "cOjwYsddCkID"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running the chatbot\n",
        "while True:\n",
        "  message = input(\"\")\n",
        "  result = pred_class(message, words, classes)\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "f97ti7gWBXqj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7TJ-k8C_1Sc",
        "outputId": "817b671c-0473-4102-a131-be5d86da9122"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['about',\n",
              " 'complaint',\n",
              " 'createaccount',\n",
              " 'goodbye',\n",
              " 'greeting',\n",
              " 'help',\n",
              " 'name',\n",
              " 'thanks']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('chatbot.pickle','wb') as f:\n",
        "  pickle.dump(model,f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obbBsuNqZRiy",
        "outputId": "63573090-edab-49b3-9100-0b9076f2af4c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://8e804bd1-a613-4da4-8114-47a2ee1f0f14/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('words.txt', 'w') as fp:\n",
        "    for item in words:\n",
        "        # write each item on a new line\n",
        "        if words.index(item) == len(words)-1:\n",
        "          fp.write(\"%s\" % item)\n",
        "        else:\n",
        "          fp.write(\"%s,\" % item)\n",
        "        "
      ],
      "metadata": {
        "id": "590gyf84bw6z"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(f\"saved_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed9APsP4DPOW",
        "outputId": "da092cff-b714-414c-ae1c-470a08b179e7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('saved_model') "
      ],
      "metadata": {
        "id": "P-o6VwLEElO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_version=1\n",
        "model.save(f\"{model_version}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rDlgwCkHFU9",
        "outputId": "238c3432-11d6-48a1-bc9b-2a24bed393d7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: 1/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -czvf model.tar.gz 1/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXuTMJJ5FBrh",
        "outputId": "085007b8-8745-421c-c3c4-afe6bc37404e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/\n",
            "1/assets/\n",
            "1/variables/\n",
            "1/variables/variables.index\n",
            "1/variables/variables.data-00000-of-00001\n",
            "1/saved_model.pb\n",
            "1/keras_metadata.pb\n"
          ]
        }
      ]
    }
  ]
}